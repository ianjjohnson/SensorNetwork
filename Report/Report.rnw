\documentclass[12pt]{report}
\usepackage[margin=0.9in]{geometry} 
\usepackage{enumitem}
\usepackage{float}
\usepackage{graphicx, subfig}
\setlist{noitemsep}

\begin{document}

\begin{titlepage}
\vspace*{\fill}
\begin{center}
      {\Huge Linear-Time Computation of High-Coverage Backbones for Wireless Sensor Networks}\\[1.5cm]

      {\Large Ian Johnson}\\[1.0cm]
      
      {\Large Southern Methodist University}\\[0.2cm]
      {\Large CSE 7350 - Algorithm Engineering}\\[0.2cm]
      {\Large Professor Lee McFearin}\\[1.0cm]
      \today
\end{center}
\vspace*{\fill}
\end{titlepage}


  
\tableofcontents

  

\chapter{Executive Summary}
\section{Introduction}
Widely distributed networks of sensors which relay information to one another, called sensor networks, are an area of interest for many natural and computer scientists. Because inexpensive sensors can easily be distributed around a physical area, sensor networks are a viable strategy for a wide variety of applications. For example, if one were interested in measuring rainfall over the surface of the earth, a globally-distributed network of rainfall-measuring sensors could be used. Connecting these sensors in an efficient and fault-tolerant way in a wireless environment is a difficult problem \textsuperscript{[1],[2]}. In order to minimize energy consumption for the overall network of sensors and maximize node availability across the network, it is useful to define a set of nodes, called a backbone, which are responsible for transmitting data across the entire network. One practical strategy for computing backbones is using graph coloring algorithms \textsuperscript{[3],[4],[5]}

In this report, we present an implementation of a graph-coloring algorithm to compute a high-coverage backbone for a wireless sensor network. The algorithm, given an randomized distribution of points (which represent sensors in a physical space) identifies a subset of points which provide maximal connectivity to the remaining points. This computation is performed in linear time with respect to the sum of the number of points and the number of connections between points. A smallest-last vertex ordering is used to compute a coloring of the graph, and high-frequency pairs of colors are evaluated as network backbones. The resulting networks, along with their backbones are visualized for a unit square distribution, a unit disk distribution, and a spherical distribution. Table \ref{summaryTable} shows the performance of the algorithm for a set of benchmark cases. The algorithm we present performs well against a set of proposed benchmarks.

\begin{table}[H]
\centering
\begin{tabular}{| c | c | c | c | c | c | c |}
\hline
ID & Distribution  & $N_{total}$ & E & R & $N_{covered}$ & Percent Coverage\\
\hline
1 & Square & 1000 & 32 & 0.101 & 992 & 0.992\\
2 & Square & 4000 & 64 & 0.071 & 3993 & 0.998\\
3 & Square & 16000 & 64 & 0.036 & 15976 & 0.998\\
4 & Square & 64000 & 64 & 0.018 & 63840 & 0.998\\
5 & Square & 64000 & 128 & 0.025 & 63960 & 0.999\\
6 & Disk & 4000 & 64 & 0.063 & 3992 & 0.998\\
7 & Disk & 4000 & 128 & 0.089 & 3995 & 0.999\\
8 & Sphere & 4000 & 64 & 0.253 & 3988 & 0.997\\
9 & Sphere & 16000 & 128 & 0.179 & 15950 & 0.996\\
10 & Sphere & 64000 & 128 & 0.089 & 61215 & 0.957\\
\hline
\end{tabular}
\caption{A summary of the results of each of the 10 benchmark data sets}
\label{summaryTable}
\end{table}

To acheive linear time complexity for backbone selection, a number of operations has to be performed in linear time. Most non-trivially, the smallest last first vertex ordering must be computed in linear time. The algorithm used to achieve this is described in Matula, D.W, Beck, L.L 1983 \textsuperscript{[6]}.


\section{Programming Environment}

The processes of random point generation, edge computation, and graph coloring are performed using Python 3, and use the Collections package from Python \textsuperscript{[7]}. All computations were performed on one of two test machines. The first machine is a 2015 MacBook Pro running MacOS Sierra with a 2.5Ghz Intel I7 processor and 16GB of RAM. The second machine, which was used only for testing, and not in the generation of this final report, is a custom-built tower running Ubuntu 14.04 with an overclocked Intel I7 4770k processor and 32GB of RAM. However, a workhorse machine such as this one is not required to reproduce the output of our analyses. 

Beyond the Python standard libary (Collections), no additional Python packages or 3rd party code were used. However, rendering was performed using the Processing framework \textsuperscript{[8]}. This framework provides a simple interface for rendering 2D and 3D point distributions, as well as connections between those points.

When the largest benchmark test was performed, the process consumed 100\% of available CPU clock cycles (it is single threaded, so this is the maximum possible usage), and 1.24GB of RAM. The benchmark in question is 64000 nodes with an average degree of 128 in a spherical distribution. Visualizing this benchmark would be computationally infeasible, as a significant amount of VRAM would be required. Considering the considerable size of the benchmark dataset, 1.24GB of RAM is a reasonable memory expenditure.

One final consideration with respect to the performance of the algorithm is that the entire process is run in Python inside the Processing environment, to make interfacing with the graphics library more simple. This adds considerable overhead to the total time it takes to perform computations on the datasets. However, because this is a constant overhead, this will not interfere with the measurements of computation time for the benchmark datasets relative to eachother.


\section{References}

\quad [1] Mahfoudh, Chalhoub, Minet, Misson, Amdouni, Node Coloring and Color Conflict Detection in Wireless Sensor Networks, \textit{Future Internet}, 2010, 469-504\\

[2] Akyildiz, Ian F., et al. "Wireless sensor networks: a survey." Computer networks 38.4 (2002): 393-422. \\

[3] Clark, Brent N., Charles J. Colbourn, and David S. Johnson. "Unit disk graphs." Discrete mathematics 86.1-3 (1990): 165-177. \\

[4] Cardei, Mihaela, et al. "Wireless sensor networks with energy efficient organization." Journal of Interconnection Networks 3.03n04 (2002): 213-229. \\

[5] Gandham, Shashidhar, Milind Dawande, and Ravi Prakash. "Link scheduling in wireless sensor networks: distributed edge-coloring revisited." Journal of Parallel and Distributed Computing 68.8 (2008): 1122-1134. \\

[6] Matula, D.W, Beck, L.L, Smallest-Last Ordering and Clustering and Graph Coloring Algorithms, \textit{Journal of the Association for Computing Machinery}, July 1983, 421-427\\

[7] Python Software Foundation. Python Language Reference, version 3.4. Available at http://www.python.org\\

[8] Reas, C. and Fry, B. Processing: programming for the media arts (2006). Journal AI and Society, volume 20(4), pages 526-538, Springer
\newpage
\setcounter{section}{0}

\chapter{Wireless Sensor Network Backbone Report}
\section{Reduction to Practice}
\subsection{Data Structure Design}
A number of data structures are utilized in the process of generating points, computing edges, coloring the graph, and identifying backbones. They are, in order of appearance in the algorithm:
\begin{itemize}
\item Points Container
\item Adjacency List
\item Colors
\item Backbones
\end{itemize}

\subsubsection{Points Container}
The points container is a simple 2D python list, where each list at index $i$ in the main list is a set of coordinates for a point. Therefore, if the points container is an $MxN$ matrix, then there are $M$ points in an $N$-dimensional space. The generalization of a 2D list allows for $N$-dimensional points to be used at any point in the algorithm, since the 2-or-3-dimensionality of the points is not ever explicitly required. This becomes very useful when switching from the Disk and Square distribution to the Spherical distribution.

\subsubsection{Adjacency List}
The adjacency list, like the points container, is a simple 2D python list. However, it is not a rectangular matrix, like the points container. The list at index $i$ in the main list represents a list of the indeces of nodes in the points container which are connected to the node at index $i$ in the points container. For example, if the 5th list in in the adjacency list contained a 1, a 2, and a 7, then the node whose location is encoded by the 5th list in the points container would be connected to the nodes at indeces 1, 2, and 7 of the points container. From this point forward, the set of coordinates at the $i$th index of the points container will be refered to as the $i$th point.

\subsubsection{Colors}
To store the colors of the points, a 1D python list is used which is parallel to the points container. At index $i$ in the colors list sits a value representing the color of the $i$th point.

\subsubsection{Backbones}
The set of backbones is stored using a simple 2D python list. The $i$th element in the 2D list represents the $i$th backbone, and contains a list of the indeces of the nodes in the $i$th backbone. For example, if the 3rd list in the backbone set had a 1, a 3, and a 5, then the 3rd backbone would be the set of points {1,3,5}.


\subsection{Algorithm Description}
The overall backbone selection algorithm can be split up into a number of parts, each of which will be explored in further sections:
\begin{itemize}
\item Random Point Selection
\item Edge Discovery
\item Graph Coloring
\item Backbone Selection
\end{itemize}

\subsubsection{Random Point Selection}
The process of random point selection is slightly different for each of the distributions tested in this report:

\begin{itemize}
\item \textbf{Unit Square}: For the unit square distribution, random point selection is performed by computing two random values between 0 and 1 and using those values as the x and y coordinates of the point. This is iterated until the desired number of points is produced.
\item \textbf{Unit Disk}: For the unit disk distribution, the same procedure from the unit square is used (where the random values can fall between -1 and 1, in this case); however, if points fall outside of a unit disk ($x^2 + y^2 > 1$), then the x and y coordinates are re-randomized until they do fall within the disk. The process still ensures that the desired number of points is produced.
\item \textbf{Unit Sphere}: For the unit sphere distribution, three random values between -1 and 1 are chosen and assigned to the x, y, and z coordinates of the point. The point is then projected onto the surface of the sphere by dividing each of the 3 coordinates by $\sqrt{x^2 + y^2 + z^2}$.
\end{itemize}

\subsubsection{Edge Discovery}
Edge discovery is performed using a bucket strategy, so that the algorithm can run in linear time (this is discussed in more detail in the upcoming \textit{Algorithm Engineering} section). In order to detect edges, the points are partitioned into a matrix of $N^2$ buckets, where $N=\frac{1}{R}$, such that the indeces of the bucket in which a point resides is an approximation of the actual location of the point in 2 dimensions. This is performed using a linear pass through the points in the points list. After that, every point in every bucket is compared to every other point in the bucket, as well as all points in adjacent buckets. This guarantees that all edges are found without directly comparing every node to every other node, which is an $O(N^2)$ algorithm.

\subsubsection{Graph Coloring}
The graph coloring algorithm utilizes a smallest-last vertex ordering to perform coloring in linear time. The smallest-last vertex ordering is performed in linear time per the algorithm described in Matula, D.W, Beck, L.L 1983 \textsuperscript{[6]}. Once the smallest-last vertex ordering has been computed, the coloring is done by iterating through the smalles-last vertex ordering and, at each point, assigning the first available color to the point on the graph by looking at the colors of points adjacent to the point in question. This is a non-heuristic graph coloring algorithm and is guaranteed to find a minimal number of colors on the graph \textsuperscript{[6]}. Verification of the linear-time claim for this algorithm can be found in the upcoming \textit{Algorithm Engineering} section.

\subsubsection{Backbone Selection}
Once the graph coloring has been performed, the points in the graph are partitioned into $k$ disjoint sets of point, where $k$ is the number of colors, and the items in each set are all of the same color. The 4 sets with the highest cardinality are selected, and the 6 possible combinations of those 4 colors are identified as possible backbones. For each of those backbones, the coverage is calculated by identifying the number of vertices in the major component of the resulting graph when only the edges which include an item from the backbone as a vertex are included in the graph. Then, the top 3 backbones based on coverage are selected and used as the backbones. 

\subsection{Algorithm Engineering}
Each part of the algorithm is optimized for performance. The various optimizations that were used are explained below.

\subsubsection{Random Point Selection}
The random point selection is optimized for the spherical distribution of points by projecting all points in a random cube of space onto the surface of the sphere. This guarantees that no points are wasted, or thrown away. For the square distribution of points, it is guaranteed that no points will be thrown away because the range of the possible results of the random location is equal to the range of possible locations on the unit square. The unit disk is slightly sub-optimal when compared to the other distributions, as it occasionally throws away points that are generated and fall outside of a unit disk. However, only approximately $\frac{1}{4}$ of points are thrown away, so the algorithm, on average still runs in linear time with respect to the number of vertices, just with a slighly higher constant.

\subsubsection{Edge Discovery}
Edge discovery, through the use of the the bucket algorithm, is performed in $\theta(|E| + |V|)$ time, where $|E|$ is the number of edges, and $|V|$ is the number of vertices. This is considered linear time with respect to the sum of the cardinalities of the sets of edges and vertices. Figure 2.1 shows the actual running time of the algorithm compared to the sum of the number of edges and vertices for each of the 10 benchmark sets.

<<echo=FALSE, fig=TRUE, height=5>>=
eplusv <- c(1000*33, 4000*65, 16000*65, 64000*65, 64000*129, 4000*65, 4000*129, 4000*65, 16000*129, 64000*129)
edges <- c(360, 910, 2799, 10569, 21818, 811, 1607, 856, 5656, 21286)
ordering <- c(154, 583, 3045, 20847, 40356, 570, 936, 555, 7891, 39742)
coloring <- c(21, 98, 545, 2154, 3731, 122, 252, 116, 1220, 3927)

plot(eplusv, edges, xlab =  "|E| + |V|", ylab = "Time (ms)", main = "Edge Discovery Time vs (|E| + |V|)")
abline(lm(edges ~ eplusv))
@
\begin{center}
\textit{Figure 2.1 - Edge discovery time vs (|E| + |V|)}
\end{center}

A cursory look at Figure 2.1 shows that there is, in fact, a linear relationship between running time and $(|E| + |V|)$. This linear relationship for the 10 benchmark sets can be treated as operational proof that the edge discovery algorithm is, in fact, $\theta(|E| + |V|)$.

\subsubsection{Smallest-Last Vertex Ordering}

Figure 2.2 shows a plot of the runtime for the smallest-last vertex ordering vs $(|E| + |V|)$. Much like figure 2.1, it shows a linear relationship between the two. Notice, however, that the slope of the regression line in this plot is much larger. This suggests that the constant in the $(|E| + |V|)$ function that represents the run-time of the ordering is much higher than that of the edge discovery process. The linear relationship between the two plotted factors provide proof that the smallest-last vertex ordering is, in fact $\theta(|E| + |V|)$. A mathematical proof of this can be found in Matula, D.W, Beck, L.L 1983 \textsuperscript{[6]}.

<<echo=FALSE, fig=TRUE, height=5>>=
plot(eplusv, ordering, xlab =  "|E| + |V|", ylab = "Time (ms)", main = "Smallest-Last Vertex Ordering Time vs (|E| + |V|)")
abline(lm(ordering ~ eplusv))
@
\begin{center}
\textit{Figure 2.2 - Smallest-last vertex ordering time vs (|E| + |V|)}
\end{center}

\subsubsection{Graph Coloring}

The graph coloring process, like the previously described processes, runs in linear time. Figure 2.3 shows the coloring time vs $(|E| + |V|)$, and, once again, identifies a linear relationship between the two. In this case, the constant on the function of  $(|E| + |V|)$ is much smaller than it was for the previous two algorithms. In fact, a cursory look at Figures 2.3 and 2.2 shows that the graph coloring process, on average, takes about $\frac{1}{10}$ of the time of the smallest-last vertex ordering process. The strong linear correlation between graph coloring time and $(|E| + |V|)$ demonstrates that the graph coloring algorithm is $\theta(|E| + |V|)$.

<<echo=FALSE, fig=TRUE, height=5>>=
plot(eplusv, coloring, xlab =  "|E| + |V|", ylab = "Time (ms)", main = "Coloring Time vs (|E| + |V|)")
abline(lm(coloring ~ eplusv))
@
\begin{center}
\textit{Figure 2.3 - Coloring time vs (|E| + |V|)}
\end{center}


\subsubsection{Backbone Selection}
The backbone selection process is quite simple, and therefore has little algorithmic complexity. The top colors are selected (this is performed by performing a bucket sort on the color lists based on their cardinalities, a $\theta(N)$ operation), and then the 6 backbones are computed. The actual computatioon of the coverages of the backbones is performed using a depth-first search, which is also a $\theta(N)$ operation. The top backbones are then selected, once again, using a bucket sort and extracting the top 3 (this is also $\theta(N)$). Because the backbone selection process is comprised of 3 individual $\theta(N)$ operations, the entire process is $\theta(N)$. Since it is well-known that all 3 of the described processes do, in fact, run in linear time, no graph of the running time of this operation is provided.

\subsection{Verification Walkthrough}
For the graph shown in Figure 2.4, the smallest-last ordering, coloring, and backbone selection algorithms will be demonstrated. For this graph, there are 20 nodes ($N=20$), the radius of each node is 0.4 ($R=0.4$), and the average degree of each node is $6$.

\includegraphics[width=6cm, height=9cm]{verification1}
\includegraphics[width=9cm, height=9cm]{verification0}
\begin{center}
\textit{Figure 2.4 - Adjacency list and visualization of reference graph for verification}
\end{center}

To perform the smallest-last ordering, a set of degree buckets will be used. These will be visualized using a table:

\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|}
\hline
Degree & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11\\
\hline
Vertices & \\
\hline
\end{tabular}

\subsection{Algorithm Efficacy}

\section{Benchmark Results}

\end{document}




              